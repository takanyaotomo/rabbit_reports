{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "深層学習day4.ipynb",
      "provenance": [],
      "mount_file_id": "1BaipDJkmXrtmdCpvQafbs69PfVoLn4Uz",
      "authorship_tag": "ABX9TyPwyRE+Z5zhH4iqgyVe4pBo"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dj0H-mOo8N-D"
      },
      "source": [
        "***\n",
        "# 深層学習day4\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkIhq4JI8Rnj"
      },
      "source": [
        "## Section1: 強化学習\n",
        "+ 教師あり学習・教師なし学習とは別の枠組み\n",
        "+ 行動の結果として得られる利益が最大化されるように行動を改善していく\n",
        "+ 答えがある訳ではなく，試行錯誤から答えを導き出す\n",
        "+ キーワード\n",
        "  + エージェント：行動を起こす主体\n",
        "  + 環境：エージェントが行動する対象\n",
        "  + 行動：エージェントが行う行為\n",
        "  + 状態：エージェントが置かれている状況\n",
        "  + 方策：エージェントが行う行為の実施ルール\n",
        "  + 報酬：行動に対する即時的な結果\n",
        "  + 収益：未来方向の報酬の和\n",
        "  + 価値：エージェントが行った行動の得られる収益の期待値\n",
        "+ 探索と利用のトレードオフ\n",
        "  + 試行錯誤（ランダム）な行動だけでは学習が進まない\n",
        "  + 学習結果のみで行動すると新たな発見が生まれない\n",
        "+ 強化学習の学習：優れた方策を見つけることが目標．計算量が問題だった\n",
        "  + Q学習：行動するたびに更新\n",
        "  + 関数近似法：価値観数や方策関数を関数近似⇒ニューラルネットワークを使う\n",
        "    + 方策：方策関数π(s, a)  \n",
        "      状態と行動をパラメータとして行動の確率を求める\n",
        "    + 価値：  \n",
        "      状態価値関数V(s)：状態をパラメータとして価値を求める  \n",
        "      行動価値関数Q(s, a)：状態と行動をパラメータとして価値を求める\n",
        "+ 方策勾配法\n",
        "$$\n",
        "\\pi(s, a | \\theta)\n",
        "$$\n",
        "について，\n",
        "$$\n",
        "\\theta^{t+1} = \\theta^{t} + \\epsilon \\nabla_{\\theta} J(\\theta) \\\\\n",
        "\\nabla_{\\theta} J(\\theta) = \\nabla_{\\theta}\\sum_{a \\in A} \\pi_{\\theta}(a|s)Q^{\\pi}(s, a) =\n",
        "E_{\\pi}\\left[ Q^{\\pi}(s, a)\\nabla_{\\theta} \\log \\pi_{\\theta}(a|s)\\right ]\n",
        "$$\n",
        "現在の状態で複数の行動が選択できる．それぞれの行動について価値が計算できる．全ての行動パターンについての価値の総和を計算している\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "【参考文献】\n",
        "1. 牧野ら編著『これからの強化学習』森北出版 2016.10\n",
        "2. 曽我部東馬著『強化学習アルゴリズム入門』オーム社 2019.05"
      ],
      "metadata": {
        "id": "rNbpKDaWrNOV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k80nzH298c9q"
      },
      "source": [
        "## Section2: AlphaGo\n",
        "+ Alpha Go Lee\n",
        "  + 畳み込みニューラルネットワークで方策関数と価値関数を学習\n",
        "  + 方策関数：PolicyNet\n",
        "    + 盤面特徴入力(H:19,W:19,C:48)\n",
        "    + 畳み込み層(K:5×5,stride:1, zeropad:2, F:192)\n",
        "    + 活性化関数(ReLU)\n",
        "    + (畳み込み層+ReLU)×11　(K:3×3,stride:1, zeropad:1, F:192)\n",
        "    + 畳み込み層(K:1×1,stride:1,F:1)\n",
        "    + 活性化関数(SoftMax)\n",
        "    + Policy出力(19×19)\n",
        "  + 価値関数：ValueNet\n",
        "    + 盤面特徴入力(H:19,W:19,C:49)手番がどちらかを示すチャネルが追加\n",
        "    + 畳み込み層(K:5×5,stride:1, zeropad:2, F:192)\n",
        "    + 活性化関数(ReLU)\n",
        "    + (畳み込み層+ReLU)×11　(K:3×3,stride:1, zeropad:1, F:192)\n",
        "    + 畳み込み層(K:1×1,stride:1,F:1)\n",
        "    + 全結合(256)←ここで1次元配列に\n",
        "    + 全結合(1)\n",
        "    + 活性化関数(tanh)\n",
        "    + Value出力(-1～1)\n",
        "  + 学習のステップ\n",
        "    + PolicyNet同士で対局シミュレーションを実施し，その結果を用いて方策勾配法でパラメータを更新\n",
        "    + ValueNetでは，対局シミュレーションの勝敗を教師データとして学習を進める．  \n",
        "      対局シミュレーションは途中までを作成済みのPolicyNetを使い，途中から強化学習で生成したPolicyNetで終局まで進め，その勝敗を報酬とする\n",
        "    + RollOutPolicy  \n",
        "      PolicyNetよりも早い計算処理できる線形の方策関数．\n",
        "    + モンテカルロ木探索：価値関数\n",
        "    1. 人間対人間の棋譜を教師データとしてPolicyNetとRollOutPolicyで学習する\n",
        "    2. 強化学習でPolicyNetを学習\n",
        "    3. 強化学習でValueNetを学習\n",
        "  + モンテカルロ木探索の詳細  \n",
        "    現局面をルートノードとして制限時間内に探索を繰り返し行う．最も多く探索された経路を選択する．\n",
        "    + Selection：リーフノードに至るまでノード選択を繰り返す．選択の基準は，行動価値関数と探索初期にランダム生成をもたせる項との和が最大になるもの．\n",
        "    + Expansion：リーフノードの到達回数が設定回数を超えた場合，新たなノードを追加する．新たなノードの選択確率はRollOutPolicyで計算される．\n",
        "    + Evaluation：リーフノードのValueNet値が未評価であれば評価する．\n",
        "    + Backup：ValueNetの評価後に探索回数，評価回数，探索時勝利回数，評価時勝利回数，行動価値関数の更新を行う\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Alpha Go Zero\n",
        "  + 教師あり学習を使わず，強化学習のみ\n",
        "  + 特徴入力からヒューリスティックな(人の判断による)要素を排除\n",
        "  + PolicyNetとValueNetを統合\n",
        "  + ResidualNetの導入\n",
        "  + モンテカルロ木探索でのRollOutシュミレーションを削除  \n",
        "  <br>  \n",
        "  + PolicyValueNet\n",
        "    + 盤面特徴入力(H:19,W:19,C:17)\n",
        "    + 畳み込み層(K:3×3,stride:1,F:256)\n",
        "    + バッチ正規化\n",
        "    + 活性化関数(ReLU)\n",
        "    + ResidualBlock×39（⇒アンサンブル効果がある？）\n",
        "      + 畳み込み層(K:3×3,stride:1,F:256)\n",
        "      + バッチ正規化\n",
        "      + 活性化関数(ReLU)\n",
        "      + 畳み込み層(K:3×3,stride:1,F:256)\n",
        "      + バッチ正規化\n",
        "      + 入力元データのショートカット結合(勾配消失爆発問題対策)\n",
        "      + 活性化関数(ReLU)\n",
        "    + ネットワークの分岐\n",
        "      1. 方策関数用\n",
        "        + 畳み込み層(K:1×1,stride:1,F:2)\n",
        "        + バッチ正規化\n",
        "        + 活性化関数(ReLU)\n",
        "        + 全結合(362)\n",
        "        + 活性化関数(SoftMax)\n",
        "        + Policy出力\n",
        "      2. 価値関数用\n",
        "        + 畳み込み層(K:1×1,stride:1,F:1)\n",
        "        + バッチ正規化\n",
        "        + 活性化関数(ReLU)\n",
        "        + 全結合(256)\n",
        "        + 活性化関数(ReLU)\n",
        "        + 全結合(1)\n",
        "        + 活性化関数(tanh)\n",
        "        + Value出力(-1～1)\n",
        "<br>  \n",
        "  + 学習手法\n",
        "    1. 自己対局\n",
        "      + モンテカルロ木探索を用いる\n",
        "      + 30手目まではランダム\n",
        "      + 局面ごとのシミュレーション回数，勝敗を記録\n",
        "    2. 学習\n",
        "      + PolicyValueNetによる学習\n",
        "      + Policy部分ではクロスエントロピー誤差，Value部分では平均二乗誤差を利用\n",
        "    3. ネットワーク更新\n",
        "      + 現状のネットワークと学習後のネットワークで対局を実施\n",
        "      + 勝率が高い方のネットワークで更新"
      ],
      "metadata": {
        "id": "w-RAkG9yLoKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "【参考文献】\n",
        "1. 牧野ら編著『これからの強化学習』森北出版 2016.10\n",
        "2. 曽我部東馬著『強化学習アルゴリズム入門』オーム社 2019.05\n",
        "3. 久保隆宏著『Pythonで学ぶ強化学習　改訂第2版』講談社 2019.09"
      ],
      "metadata": {
        "id": "LODFuuB1bGCw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmfddZ6E8hVz"
      },
      "source": [
        "## Section3: 軽量化・高速化技術\n",
        "+ 分散深層学習  \n",
        "  モデルが複雑に，データ量の増加で計算量が年10倍増加しているがコンピュータの性能は18ヶ月で2倍⇒複数のコンピュータで同時に処理する\n",
        "  + ワーカー：学習させる装置の単位．ワーカーは個別のコンピュータというわけではなく，GPUを1ワーカーとしてもよい．スマートフォンもワーカーとなり得る．\n",
        "  + データ並列化：学習モデルを複製し，ワーカーを複数用意．データを分割してそれぞれのワーカーに計算させる．\n",
        "    + 同期型：各ワーカーの演算を集計し，平均値で更新．学習スピードは早いが処理スピードは遅い．\n",
        "    + 非同期型：演算の結果は同期をとらず随時更新，各ワーカーからの演算結果を次の演算のパラメータとして利用．処理スピードは早いが学習スピードは遅い\n",
        "  + モデル並列：モデルを複数のパーツに分割して各ワーカーで処理．ネットワーク分岐のあるモデルと相性が良い．各分割モデルを各GPUに配置することが多い．パラメータ数が多いほど効率が上がる．\n",
        "+ GPU高速化  \n",
        "  低性能だが単純な並列処理が得意なコアが多数．本来は画像処理に利用⇒ニューラルネットワークと相性が良い\n",
        "  + GPGPU(General Purpose on GPU)\n",
        "    + CUDA: NVIDIA用．Deep Learning用に提供されている\n",
        "    + OpenCL: 他社GPUでも使用可能．Deep Learningに特化はしていない．  \n",
        "\n",
        "    ※GPGPUを意識せずにTensorflowやPyTorch内で利用できる\n",
        "+ 軽量化\n",
        "  + 量子化(Quantization)  \n",
        "    ネットワークが大きくなるとパラメータが多いので演算量とメモリ消費量が大きくなるので，数値の精度を落とすことで演算量とメモリ消費量を削減（16bitにしてもネットワークの精度は大きく変わらない）\n",
        "+ 蒸留(Knowlegde Distillation)  \n",
        "  学習済みの精度の高いモデルから軽量モデルへ知識を継承\n",
        "  + 教師モデル：大きい，複雑，精度が高い\n",
        "  + 生徒モデル：軽い，単純，精度が落ちる？⇒若干落ちる？  \n",
        "\n",
        "  学習済み教師モデルからの出力(固定)と生徒モデルからの出力を結合して生徒モデルの重みを更新\n",
        "+ プルーニング  \n",
        "  大きなネットワークの中で精度に寄与していないニューロンを削除することでモデルの軽量化，高速化\n",
        "\n",
        "  + 重みが閾値以下のニューロンを削除（削減数の割に精度は下がらない）\n",
        "  + ドロップアウトは過学習を防ぐため，重み更新の度に一部のニューロンをランダムに不活性化させる\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdBH0ZEn8l3K"
      },
      "source": [
        "## Section4: 応用技術\n",
        "2017年頃に画像認識の技術は精度面で完成されており，今は計算量の削減方法を改善している\n",
        "+ 一般的な畳み込み層\n",
        "  + 入力層：(H, W, C)\n",
        "  + 畳み込み層：(K, K, C)×F\n",
        "  + 出力層：(H, W, F)\n",
        "  + 畳み込み演算の計算量：H×W×K×K×C×F\n",
        "+ MobileNets  \n",
        "  Depthwise ConvolutionとPointwise Convolutionで計算量を削減\n",
        "  + Depthwise Convolution\n",
        "    + 畳み込み層のカーネルは(K, K, 1)×1で各チャネルを1枚のフィルタで処理\n",
        "    + 畳み込み演算の計算量：H×W×C×K×K（畳み込み層のチャネル数とフィルタ数分が削減）\n",
        "  + Pointwise Convolution\n",
        "    + 畳み込み層のカーネルは(1, 1, C)×Fでカーネルサイズを(1,1)に固定して処理\n",
        "    + 畳み込み演算の計算量：H×W×C×F（カーネルサイズ(K,K)が削減）\n",
        "\n",
        "  カーネルサイズとフィルタに関する演算を分割することで計算量を削減  \n",
        "  H×W×C×(K×K×F - (K×K + F))  \n",
        "\n",
        "<br>\n",
        "\n",
        "【確認テスト】  \n",
        "Depthwise Convolitionはチャネル毎に空間方向へ畳み込む。すなわち、チャネル毎にD_K×D_K×１のサイズのフィルターをそれぞれ用いて計算を行うため、その計算量は（？）となる。  \n",
        "【解答】H×W×C×D_k×D_k \n",
        "\n",
        "<br>  \n",
        "\n",
        "【確認テスト】  \n",
        "次にDepthwise Convolutionの出力をPointwise Convolutionによってチャネル方向に畳み込む。すなわち、出力チャネル毎に１×１×Mサイズのフィルターをそれぞれ用いて計算を行うため、その計算量は（？）となる。  \n",
        "【解答】H×W×C×M"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ DenseNet\n",
        "  + Denseブロック  \n",
        "    入力と畳み込み層との演算結果と入力データを別チャネルとして結合する(スキップコネクション）\n",
        "    + バッチ正規化\n",
        "    + 活性化関数(ReLU)\n",
        "    + 畳み込み演算(3×3)  \n",
        "\n",
        "    畳み込み演算する度に出力チャネル数分だけチャネル数が増加．  \n",
        "  \n",
        "  + Transition Layer: Denseブロックで増えたチャネル数を入力チャネル数に戻す処理が行われる\n",
        "    + 畳み込み層(1,1)\n",
        "    + AveragePooling層(2,2)  \n",
        "\n",
        "  l層への入力をCl，初期入力チャネル数をk0，成長率(growth rate)をkとすると\n",
        "  $$\n",
        "  C_l = k_0 + k(l-1)\n",
        "  $$\n",
        "となる\n",
        "\n",
        "  + ResNetとの違い\n",
        "    + Denseブロック：前方各層の入力が後方層の入力となる⇔Ressidualブロック：前1層の入力のみ後方層の入力となる\n",
        "    + Denseブロックにはgrowth rateのチャネル数だけ層が増加していく"
      ],
      "metadata": {
        "id": "5bTDNKlxpzt_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Batch Normalization\n",
        "  + レイヤに入力されるデータをミニバッチ単位で正規化(平均0，分散1)\n",
        "  + 処理環境によってバッチサイズ(数枚～数十枚)が異るので評価しづらい\n",
        "  + BatchNorm：サンプルデータ全体の同一チャネルが同一分布になるように正規化\n",
        "    + ミニバッチのデータをチャネルごとにグループ化して正規化処理\n",
        "    + バッチサイズが小さいと学習が収束しないことがある\n",
        "  + LayerNorm：サンプルデータごとに同一分布になるように正規化\n",
        "    + データ1枚ごとにチャネルを統合して正規化\n",
        "    + バッチサイズによる影響はない\n",
        "    + 入力データのスケール，重み行列のスケール・シフトにロバスト\n",
        "  + InstanceNorm：各サンプルデータのチャネルごとに正規化\n",
        "    + 画像のスタイル転送やテクスチャ合成で利用\n",
        "+ Wavenet  \n",
        "  音声を生成するモデル\n",
        "  + 時系列データに対して畳み込みを適用：連続するデータに対してまとめてフィルタ処理をする\n",
        "  + Dilated convolution：間隔を開けたデータでフィルタ処理をする  \n",
        "\n",
        "<br>\n",
        "\n",
        "【確認テスト】  \n",
        "\n",
        "深層学習を用いて結合確率を学習する際に、効率的に学習が行えるアーキテクチャを提案したことがWaveNet の大きな貢献の1 つである。提案された新しいConvolution 型アーキテクチャは（？）と呼ばれ、結合確率を効率的に学習できるようになっている。  \n",
        "【解答】Dilated causal convolution  \n",
        "\n",
        "Depthwise separable convolution, Pointwise convolution ⇒ MoblieNets  \n",
        "\n",
        "Deconvolution ⇒ 畳み込み処理の逆演算，オートエンコーダでも利用\n",
        "\n",
        "\n",
        "【確認テスト】  \n",
        "（？）を用いた際の大きな利点は、単純なConvolution layer と比べて（？？）ことである。  \n",
        "【解答】パラメータ数に対する受容野が広い  \n",
        "出力を長い時系列データを元に生成することができる\n"
      ],
      "metadata": {
        "id": "_bbcjet1992w"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCiK7FQ48pjC"
      },
      "source": [
        "## Section5: Transformer\n",
        "+ Transformer = seq2seq × attention => BERT\n",
        "+ seq2seqとは系列データを入力して別の系列データを出力\n",
        "  + Encoder-Decoderモデル\n",
        "    + Encode：内部状態に変換\n",
        "    + Decode：系列データに変換\n",
        "  + 翻訳（日本語→英語）・音声認識（音声波形→テキスト）・チャットボット（テキスト→テキスト）\n",
        "  + RNN：再帰的，時系列データをエンコード\n",
        "    + 内部状態を保持している\n",
        "  + 言語モデル：単語の並びに確率を与える（もっともらしさ：尤度）\n",
        "    + 過去の時系列データで次の事象の出現確率分布を求める  \n",
        "\n",
        "    ⇒学習がされていれば，先頭の単語を与えれば，文章が生成できる\n",
        "  + seq2seq：EncoderのRNNとDecoderのRNNとの結合 \n",
        "    + EncoderからDecoderに渡される内部状態が重要\n",
        "    + Decoderの出力と正解ラベルで教師あり学習が行われる\n",
        "  \n",
        "\n",
        "+ BLEU：Bilingual Evaluation Understudy\n",
        "  + 機械の翻訳が正解訳にどのくらい似ているのか\n",
        "  $$\n",
        "  BLEU = BP_{BLEU} \\exp\\left( \\sum_{n=1}^{N} w_n \\log p_n \\right)\\\\\n",
        "  w_n = \\frac{1}{N}~~, ~~  p_n = \\frac{\\sum i \\text{番目の学習データでの機械翻訳と正解訳で一致したn-gram数} }{\\sum i \\text{番目の学習データでの機械翻訳の全n-gram数}}\\\\\n",
        "   BP_{BLEU} := \\text{翻訳文が参照訳より短い場合のペナルティ}\n",
        "  $$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ seq2seq\n",
        "【コード分析】  \n",
        "参考文献を元に簡略化したseq2seqコードの構造を把握"
      ],
      "metadata": {
        "id": "ly-3IYxwySAC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrwkh1XE_nUE"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Study-AI/stage4/seq2seq/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optimizers\n",
        "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
        "\n",
        "from utils import Vocab\n",
        "\n",
        "torch.manual_seed(1)\n",
        "random_state = 42"
      ],
      "metadata": {
        "id": "FhoGeXkE_88U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "class Vocab(object):\n",
        "    \n",
        "    def __init__(self, word2id={}):\n",
        "        \"\"\"\n",
        "        word2id: 単語(str)をインデックス(int)に変換する辞書\n",
        "        id2word: インデックス(int)を単語(str)に変換する辞書\n",
        "        \"\"\"\n",
        "        self.word2id = dict(word2id)\n",
        "        self.id2word = {v: k for k, v in self.word2id.items()}    \n",
        "        \n",
        "    def build_vocab(self, sentences, min_count=1):\n",
        "        # 各単語の出現回数の辞書を作成する\n",
        "        word_counter = {}\n",
        "        for sentence in sentences:\n",
        "            for word in sentence:\n",
        "                word_counter[word] = word_counter.get(word, 0) + 1\n",
        "\n",
        "        # min_count回以上出現する単語のみ語彙に加える\n",
        "        for word, count in sorted(word_counter.items(), key=lambda x: -x[1]):\n",
        "            if count < min_count:\n",
        "                break\n",
        "            _id = len(self.word2id)\n",
        "            self.word2id.setdefault(word, _id)\n",
        "            self.id2word[_id] = word\n",
        "```\n",
        "+ Vocabクラス\n",
        "  + インスタンス生成時に引数として渡した辞書オブジェクトを元に2種類の単語辞書を生成  \n",
        "  （単語辞書に予約語を追加する）\n",
        "  + build_vocabメソッド\n",
        "    + 引数のsentencesは単語を要素とした2次元配列であることを想定\n",
        "    + 1つ目のループでsentences全体での単語の出現回数を集計\n",
        "    + 2つ目のループで引数で渡された値以上の出現回数の単語を  \n",
        "    [word2id] キー：単語，　値：id番号  \n",
        "    [id2word] キー：id番号，値：単語  \n",
        "    で単語辞書を成長させる\n"
      ],
      "metadata": {
        "id": "wegmU5_O0JB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_path):\n",
        "  data = []\n",
        "  for line in open(file_path, encoding='utf-8'):\n",
        "    words = line.strip().split()\n",
        "    data.append(words)\n",
        "  return data"
      ],
      "metadata": {
        "id": "OQKzr5nuCuqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X = load_data('/content/drive/MyDrive/Study-AI/stage4/data/train.en')\n",
        "train_Y = load_data('/content/drive/MyDrive/Study-AI/stage4/data/train.ja')"
      ],
      "metadata": {
        "id": "O9rYt9b5C3qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ load_data関数\n",
        "  + ファイルの内のテキストを単語単位の2次元配列に変換\n",
        "  + 日本語テキストもスペース区切りの形式で渡されている"
      ],
      "metadata": {
        "id": "fPyGqalP5IUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, valid_X, train_Y, valid_Y = train_test_split(train_X, train_Y, test_size=0.2, random_state=random_state)"
      ],
      "metadata": {
        "id": "dsdB3K5uDV6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_TOKEN, BOS_TOKEN, EOS_TOKEN, UNK_TOKEN = '<PAD>', '<S>', '</S>', '<UNK>'\n",
        "PAD, BOS, EOS, UNK = 0, 1, 2, 3\n",
        "\n",
        "MIN_COUNT = 2  \n",
        "word2id = {PAD_TOKEN: PAD, BOS_TOKEN: BOS, EOS_TOKEN: EOS, UNK_TOKEN: UNK}\n",
        "\n",
        "vocab_X = Vocab(word2id=word2id)\n",
        "vocab_Y = Vocab(word2id=word2id)\n",
        "vocab_X.build_vocab(train_X, min_count=MIN_COUNT)\n",
        "vocab_Y.build_vocab(train_Y, min_count=MIN_COUNT)"
      ],
      "metadata": {
        "id": "gvnaZDVKD8H3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 英語と日本語の単語辞書を生成\n",
        "+ 予約語は4つ\n",
        "+ 文書ファイル内に2回以上出現する単語を辞書に登録"
      ],
      "metadata": {
        "id": "qMSF6CPf6MWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_to_ids(vocab, sentence):\n",
        "  ids = [vocab.word2id.get(word, UNK) for word in sentence]\n",
        "  ids += [EOS]\n",
        "  return ids"
      ],
      "metadata": {
        "id": "AiswGKCkK6WV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X = [sentence_to_ids(vocab_X, sentence) for sentence in train_X]\n",
        "train_Y = [sentence_to_ids(vocab_Y, sentence) for sentence in train_Y]\n",
        "valid_X = [sentence_to_ids(vocab_X, sentence) for sentence in valid_X]\n",
        "valid_Y = [sentence_to_ids(vocab_Y, sentence) for sentence in valid_Y]"
      ],
      "metadata": {
        "id": "iljeY2RoLCl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ sentence_to_ids関数\n",
        "  + 単語辞書vocabを基にテキストデータのsentenceをid番号化した配列に変換\n",
        "  + 文末に[EOS]を追加\n",
        "+ 英語と日本語の文書について学習用と検証用それぞれに対しid番号化する"
      ],
      "metadata": {
        "id": "je3y3g927N1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_seq(seq, max_length):\n",
        "  res = seq + [PAD for i in range(max_length - len(seq))]\n",
        "  return res    "
      ],
      "metadata": {
        "id": "J4JBffwrLKns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ pad_seq関数\n",
        "  + 各単語列の要素数を統一するためpadding"
      ],
      "metadata": {
        "id": "GykzCWvM-OaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "class DataLoader(object):\n",
        "\n",
        "  def __init__(self, X, Y, batch_size, shuffle=False):\n",
        "    self.data = list(zip(X, Y))\n",
        "    self.batch_size = batch_size\n",
        "    self.shuffle = shuffle\n",
        "    self.start_index = 0\n",
        "    \n",
        "    self.reset()\n",
        "  \n",
        "  def reset(self):\n",
        "    if self.shuffle:\n",
        "      self.data = shuffle(self.data, random_state=random_state)\n",
        "    self.start_index = 0\n",
        "  \n",
        "  def __iter__(self):\n",
        "    return self\n",
        "\n",
        "  def __next__(self):\n",
        "    if self.start_index >= len(self.data):\n",
        "      self.reset()\n",
        "      raise StopIteration()\n",
        "\n",
        "    seqs_X, seqs_Y = zip(*self.data[self.start_index:self.start_index+self.batch_size])\n",
        "\n",
        "    seq_pairs = sorted(zip(seqs_X, seqs_Y), key=lambda p: len(p[0]), reverse=True)\n",
        "    seqs_X, seqs_Y = zip(*seq_pairs)\n",
        "    lengths_X = [len(s) for s in seqs_X]\n",
        "    lengths_Y = [len(s) for s in seqs_Y]\n",
        "    max_length_X = max(lengths_X)\n",
        "    max_length_Y = max(lengths_Y)\n",
        "    padded_X = [pad_seq(s, max_length_X) for s in seqs_X]\n",
        "    padded_Y = [pad_seq(s, max_length_Y) for s in seqs_Y]\n",
        "\n",
        "    batch_X = torch.tensor(padded_X, dtype=torch.long).transpose(0, 1)\n",
        "    batch_Y = torch.tensor(padded_Y, dtype=torch.long).transpose(0, 1)\n",
        "\n",
        "    self.start_index += self.batch_size\n",
        "\n",
        "    return batch_X, batch_Y, lengths_X"
      ],
      "metadata": {
        "id": "flOAXOutLPJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ DataLoaderクラス\n",
        "  + バッチサイズの指定が可能\n",
        "  + データのシャッフルが可能\n",
        "  + 学習モデルで利用可能なデータ形式を出力\n",
        "    + \\__next\\__メソッド内\n",
        "      + 英語文と日本語訳との対はzip関数で保持したままの変数dataを基本に処理\n",
        "      + 単語配列のデータ長を揃えるためにpadding処理をするが，処理の効率化（padがより少なくなるように）のためにバッチ内でのデータ長のばらつきを小さくするためデータ長でソート\n",
        "      + バッチ単位でpadding処理しデータ長を揃える\n",
        "      + 学習モデルで利用可能な形式に変換"
      ],
      "metadata": {
        "id": "WqEAMuUGAVGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.embedding = nn.Embedding(input_size, hidden_size, padding_idx=PAD)\n",
        "    self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "  def forward(self, seqs, input_lengths, hidden=None):\n",
        "    emb = self.embedding(seqs)\n",
        "    packed = pack_padded_sequence(emb, input_lengths)\n",
        "    output, hidden = self.gru(packed, hidden)\n",
        "    output, _ = pad_packed_sequence(output)\n",
        "    return output, hidden"
      ],
      "metadata": {
        "id": "FYasZzElAAis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Encoderクラス\n",
        "  + インスタンス生成時のパラメータに対応したEmbedding行列とGRUユニットを生成\n",
        "  + forwardメソッド（順伝播）\n",
        "    + 入力単語列を分散表現に変換\n",
        "    + pack_padded_sequence関数で入力データをpadding前の単語列データに変換\n",
        "    + GRUユニットで演算で，出力データと隠れ層データを生成\n",
        "    + pad_packed_sequence関数で出力データをpadding後の単語列データに変換\n",
        "    + padding後の出力データと隠れ層データを戻す\n"
      ],
      "metadata": {
        "id": "XpTibZFZdOzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, hidden_size, output_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "\n",
        "    self.embedding = nn.Embedding(output_size, hidden_size, padding_idx=PAD)\n",
        "    self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "    self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, seqs, hidden):\n",
        "    emb = self.embedding(seqs)\n",
        "    output, hidden = self.gru(emb, hidden)\n",
        "    output = self.out(output)\n",
        "    return output, hidden"
      ],
      "metadata": {
        "id": "xP4qhIlXCEGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Decoderクラス\n",
        "  + インスタンス生成時のパラメータに対応したEmbedding行列，GRUユニット，線形ユニットを生成\n",
        "  + forwardメソッド（順伝播）\n",
        "    + 入力データを分散表現に変換\n",
        "    + GRUユニットで出力データと隠れ層データを生成\n",
        "    + 出力データを線形ユニットで演算\n",
        "    + 出力データと隠れ層データを戻す"
      ],
      "metadata": {
        "id": "eLKJO_fQ3uxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "  def __init__(self, input_size, output_size, hidden_size):\n",
        "    super(EncoderDecoder, self).__init__()\n",
        "    self.encoder = Encoder(input_size, hidden_size)\n",
        "    self.decoder = Decoder(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, batch_X, lengths_X, max_length, batch_Y=None, use_teacher_forcing=False):\n",
        "    _, encoder_hidden = self.encoder(batch_X, lengths_X)\n",
        "    _batch_size = batch_X.size(1)\n",
        "\n",
        "    decoder_input = torch.tensor([BOS] * _batch_size, dtype=torch.long)\n",
        "    decoder_input = decoder_input.unsqueeze(0)\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    decoder_outputs = torch.zeros(max_length, _batch_size, self.decoder.output_size)\n",
        "\n",
        "    for t in range(max_length):\n",
        "      decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
        "      decoder_outputs[t] = decoder_output\n",
        "      if use_teacher_forcing and batch_Y is not None:\n",
        "        decoder_input = batch_Y[t].unsqueeze(0)\n",
        "      else:\n",
        "        decoder_input = decoder_output.max(-1)[1]\n",
        "        \n",
        "    return decoder_outputs"
      ],
      "metadata": {
        "id": "S0h2fEumCm9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ EncoderDecoderクラス\n",
        "  + EncoderとDecoderのインスタンスを1つずつ生成\n",
        "  + forwardメソッド\n",
        "    + エンコーダインスタンスで入力データをエンコード処理し，デコーダインスタンスに渡す隠れ層データを生成\n",
        "    + デコーダインスタンスで利用する初期データを生成\n",
        "    + ループ処理内でデコード処理\n",
        "    + use_teacher_forcingを有効にしていた場合はループの入力データに正解データを使う\n",
        "    + デコード処理結果を戻す"
      ],
      "metadata": {
        "id": "r1hDAEUHUfCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mce = nn.CrossEntropyLoss(reduction='sum', ignore_index=PAD)\n",
        "def masked_cross_entropy(logits, target):\n",
        "  logits_flat = logits.view(-1, logits.size(-1))\n",
        "  target_flat = target.view(-1)\n",
        "  return mce(logits_flat, target_flat)"
      ],
      "metadata": {
        "id": "2Mh_DI5mN7NG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ masked_cross_entropy関数\n",
        "  + 予測データと正解データを引数としてクロスエントロピー誤差を計算\n",
        "  + データのサイズ変更に際し，メモリー上で要素順に並べる処理(contiguous)後のデータを受け取ることを想定\n",
        "  + paddingの要素は無視して演算"
      ],
      "metadata": {
        "id": "oRL5ACi9AKMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size_X = len(vocab_X.id2word)\n",
        "vocab_size_Y = len(vocab_Y.id2word)"
      ],
      "metadata": {
        "id": "oGmsvd4aEXCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "lr = 1e-3\n",
        "teacher_forcing_rate = 0.2\n",
        "ckpt_path = 'model.pth'\n",
        "\n",
        "model_args = {'input_size': vocab_size_X, 'output_size': vocab_size_Y, 'hidden_size': 256}"
      ],
      "metadata": {
        "id": "iMJuSmrUOGw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(batch_X, batch_Y, lengths_X, model, optimizer=None, is_train=True):\n",
        "  model.train(is_train)\n",
        "  \n",
        "  use_teacher_forcing = is_train and (random.random() < teacher_forcing_rate)\n",
        "  max_length = batch_Y.size(0)\n",
        "\n",
        "  pred_Y = model(batch_X, lengths_X, max_length, batch_Y, use_teacher_forcing)\n",
        "\n",
        "  loss = masked_cross_entropy(pred_Y.contiguous(), batch_Y.contiguous())\n",
        "  \n",
        "  if is_train:\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  \n",
        "  batch_Y = batch_Y.transpose(0, 1).contiguous().data.cpu().tolist()\n",
        "  pred = pred_Y.max(dim=-1)[1].data.cpu().numpy().T.tolist()\n",
        "\n",
        "  return loss.item(), batch_Y, pred"
      ],
      "metadata": {
        "id": "UHR_qtwoOUNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ compute_loss関数\n",
        "  + クロスエントロピー誤差の演算処理\n",
        "  + 学習モードでパラメータ更新を実行\n",
        "  + 戻り値のうち，今回利用するのはloss.item()のみ\n",
        "  + 戻り値のbatch_YとpredはBLEU計算に利用"
      ],
      "metadata": {
        "id": "zLqgpdwgJoPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_X, train_Y, batch_size=batch_size, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_X, valid_Y, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "model = EncoderDecoder(**model_args)\n",
        "optimizer = optimizers.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "jvsoJCZ7PIQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 学習用データの生成\n",
        "  + 学習用データと評価用データをバッチサイズに合わせて生成\n",
        "+ 学習モデル(EncoderDecoder)の生成\n",
        "+ 最適化手法をAdamに設定"
      ],
      "metadata": {
        "id": "xyBUY9KmN4NH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, num_epochs+1):\n",
        "  train_loss = 0.\n",
        "  #train_refs = []\n",
        "  #train_hyps = []\n",
        "  valid_loss = 0.\n",
        "  #valid_refs = []\n",
        "  #valid_hyps = []\n",
        "\n",
        "  for batch in train_dataloader:\n",
        "    batch_X, batch_Y, lengths_X = batch\n",
        "    loss, gold, pred = compute_loss(\n",
        "      batch_X, batch_Y, lengths_X, model, optimizer, \n",
        "      is_train=True\n",
        "      )\n",
        "    train_loss += loss\n",
        "    #train_refs += gold\n",
        "    #train_hyps += pred\n",
        "\n",
        "  for batch in valid_dataloader:\n",
        "    batch_X, batch_Y, lengths_X = batch\n",
        "    loss, gold, pred = compute_loss(\n",
        "      batch_X, batch_Y, lengths_X, model, \n",
        "      is_train=False\n",
        "      )\n",
        "    valid_loss += loss\n",
        "    #valid_refs += gold\n",
        "    #valid_hyps += pred\n",
        "\n",
        "  train_loss = np.sum(train_loss) / len(train_dataloader.data)\n",
        "  valid_loss = np.sum(valid_loss) / len(valid_dataloader.data)\n",
        "\n",
        "\n",
        "  print('Epoch {}: train_loss: {:5.2f} valid_loss: {:5.2f}'.format(\n",
        "      epoch, train_loss, valid_loss))\n",
        "  print('-'*40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BTK80gbOm0q",
        "outputId": "cb7e0d09-6bd3-45a1-8e4c-4ae7fa4f8b89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: train_loss: 52.09 valid_loss: 48.95\n",
            "----------------------------------------\n",
            "Epoch 2: train_loss: 44.58 valid_loss: 44.80\n",
            "----------------------------------------\n",
            "Epoch 3: train_loss: 40.05 valid_loss: 42.64\n",
            "----------------------------------------\n",
            "Epoch 4: train_loss: 37.57 valid_loss: 41.01\n",
            "----------------------------------------\n",
            "Epoch 5: train_loss: 35.00 valid_loss: 40.30\n",
            "----------------------------------------\n",
            "Epoch 6: train_loss: 33.25 valid_loss: 40.07\n",
            "----------------------------------------\n",
            "Epoch 7: train_loss: 31.83 valid_loss: 40.13\n",
            "----------------------------------------\n",
            "Epoch 8: train_loss: 30.61 valid_loss: 39.98\n",
            "----------------------------------------\n",
            "Epoch 9: train_loss: 29.09 valid_loss: 40.14\n",
            "----------------------------------------\n",
            "Epoch 10: train_loss: 28.15 valid_loss: 40.42\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 学習\n",
        "  + ミニバッチ用学習データでパラメータ更新と誤差の計算を実施\n",
        "  + ミニバッチ用評価データで学習後のモデルの誤差を計算"
      ],
      "metadata": {
        "id": "0kfUfL3ZUvdW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "【参考文献】\n",
        "1. 斎藤康毅著『セロから作るDeepLearning2 自然言語処理編』オライリー・ジャパン 2018.07\n",
        "2. 巣籠悠輔著『詳解ディープラーニング 第2版』マイナビ出版 2019.11"
      ],
      "metadata": {
        "id": "lTRdl5VwXid5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Attention\n",
        "  + seq2seqはデータを固定長ベクトルに変換するので，長い文章に弱い\n",
        "  $$\n",
        "  \\boldsymbol{c} = f(\\boldsymbol{h}_s (t-1), \\boldsymbol{x}(t))\\\\\n",
        "  \\boldsymbol{h}_t (t) = f(\\boldsymbol{h}_t (t-1), \\boldsymbol{y}(t-1), \\boldsymbol{c})\n",
        "  $$\n",
        "  + 時刻ごとに生成される隠れ状態に重みをつける⇒Attention⇒Transformer\n",
        "  $$\n",
        "  \\boldsymbol{c}(t) = \\sum_{\\tau=1}^{T} a(\\tau, t)\\boldsymbol{h}_s(\\tau)~~, ~~\n",
        "  \\sum_{\\tau=1}^{T} a(\\tau, t) = 1\\\\\n",
        "  \\boldsymbol{h}_t (t) = f(\\boldsymbol{h}_t (t-1), \\boldsymbol{y}(t-1), \\color {red}{\\boldsymbol{c}(t)})\n",
        "  $$\n",
        "  ※時刻tで，各時刻で生成されるhsの重要度(重み)が異なることを表現できる  \n",
        "  ※重要度(重み)=注意Attentionでありこの値をどのように計算するか\n",
        "$$\n",
        "a(\\tau, t) = softmax(g(\\boldsymbol{h}_s (\\tau),\\boldsymbol{h}_t (t-1)))\n",
        "$$\n",
        "  + Attentionはquery・key・valueの三つ組の構成\n",
        "    + keyに対応するqueryからvalueを取り出す  \n",
        "Scaled Dot-Product Attention  \n",
        "$$\n",
        "Attention(Q, K, V) = softmax \\left(\\frac{QK^T}{\\sqrt{d_k}} \\right ) V\n",
        "$$\n",
        "\n",
        "  + 長い文章でも性能が出せるようになる\n",
        "    + ソース・ターゲット：queryはtargetデータ，ほかはsourceデータ\n",
        "    + Self-Attention：queryもsourceデータ  \n",
        "\n",
        "$$ \n",
        "g:=\\left \\{ \n",
        "\\begin{array}{l}\n",
        "\\boldsymbol{h}_t^T \\boldsymbol{h}_s \\\\\n",
        "\\boldsymbol{h}_s^T \\boldsymbol{h}_s\n",
        "\\end{array}\\right .\n",
        "$$\n",
        "+ Transformer(Self-Attention)\n",
        "  + Encoder Decoderの構造だが，\n",
        "  + RNNを利用せずAttentionモジュールで構成\n",
        "    + 時系列ではないので単語の語順を保持できない⇒Positional Encoding\n",
        "    + デコーダで未来の単語を利用しない⇒Masked Multi-Head Attention\n",
        "  + Feed Forwardモジュール\n",
        "    + 2つの全結合層と間に活性化関数ReLUを使ったもモジュール\n",
        "$$\n",
        "u = x W_1 + b_1 \\\\\n",
        "z = relu(u)\\\\\n",
        "y = z W_2 + b_2\n",
        "$$\n",
        "  + Positional Encodingモジュール\n",
        "    + 入力データが時系列ではにので順序情報を付加する\n",
        "    + 入力データの次元数と特徴量次元数からなるマトリクスで対応\n",
        "      + 特徴量のインデックが偶数のときはsin値，奇数のときはcos値  \n",
        "$$\n",
        "PE_{(pos, 2i)} = \\sin \\left (\\frac{pos}{10000^{\\frac{2i}{d}}} \\right)~~,~~\n",
        "PE_{(pos, 2i+1)} = \\cos \\left (\\frac{pos}{10000^{\\frac{2i}{d}}} \\right)\n",
        "$$\n",
        "  \n",
        "  + Multi-Head Attention\n",
        "    + Scaled Dot-Product Attentionを複数並列処理\n",
        "    + アンサンブル学習のような効果が期待できる\n",
        "  + Masked Multi-Head Attention\n",
        "    + デコーダ内で時刻t以降の単語情報を使用しないようにMaskをかけたもの\n",
        "    + 動作自体はMulit-Head Attentionと同様\n",
        "  + （Add&Normモジュール）\n",
        "    + residual connectionと正規化処理を実行 \n"
      ],
      "metadata": {
        "id": "DLCzKHSxWIXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "【コード分析】"
      ],
      "metadata": {
        "id": "QV4dA56_O5x3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def position_encoding_init(n_position, d_pos_vec):\n",
        "    \"\"\"\n",
        "    Positional Encodingのための行列の初期化を行う\n",
        "    :param n_position: int, 系列長\n",
        "    :param d_pos_vec: int, 隠れ層の次元数\n",
        "    :return torch.tensor, size=(n_position, d_pos_vec)\n",
        "    \"\"\"\n",
        "    # PADがある単語の位置はpos=0にしておき、position_encも0にする\n",
        "    position_enc = np.array([\n",
        "        [pos / np.power(10000, 2 * (j // 2) / d_pos_vec) for j in range(d_pos_vec)]\n",
        "        if pos != 0 else np.zeros(d_pos_vec) for pos in range(n_position)])\n",
        "    position_enc[1:, 0::2] = np.sin(position_enc[1:, 0::2])  # dim 2i\n",
        "    position_enc[1:, 1::2] = np.cos(position_enc[1:, 1::2])  # dim 2i+1\n",
        "    return torch.tensor(position_enc, dtype=torch.float)"
      ],
      "metadata": {
        "id": "OqFR67SVO4mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ position_encoding_init関数\n",
        "  + 入力データと特徴量のマトリクスを生成\n",
        "    + 要素は三角関数の引数になる値を生成\n",
        "  + マトリクスの偶数列はsin，奇数列はcosで演算し要素を上書き"
      ],
      "metadata": {
        "id": "AyV4GyrIPVH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model, attn_dropout=0.1):\n",
        "        \"\"\"\n",
        "        :param d_model: int, 隠れ層の次元数\n",
        "        :param attn_dropout: float, ドロップアウト率\n",
        "        \"\"\"\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.temper = np.power(d_model, 0.5)  # スケーリング因子\n",
        "        self.dropout = nn.Dropout(attn_dropout)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, q, k, v, attn_mask):\n",
        "        \"\"\"\n",
        "        :param q: torch.tensor, queryベクトル, \n",
        "            size=(n_head*batch_size, len_q, d_model/n_head)\n",
        "        :param k: torch.tensor, key, \n",
        "            size=(n_head*batch_size, len_k, d_model/n_head)\n",
        "        :param v: torch.tensor, valueベクトル, \n",
        "            size=(n_head*batch_size, len_v, d_model/n_head)\n",
        "        :param attn_mask: torch.tensor, Attentionに適用するマスク, \n",
        "            size=(n_head*batch_size, len_q, len_k)\n",
        "        :return output: 出力ベクトル, \n",
        "            size=(n_head*batch_size, len_q, d_model/n_head)\n",
        "        :return attn: Attention\n",
        "            size=(n_head*batch_size, len_q, len_k)\n",
        "        \"\"\"\n",
        "        # QとKの内積でAttentionの重みを求め、スケーリングする\n",
        "        attn = torch.bmm(q, k.transpose(1, 2)) / self.temper  # (n_head*batch_size, len_q, len_k)\n",
        "        # Attentionをかけたくない部分がある場合は、その部分を負の無限大に飛ばしてSoftmaxの値が0になるようにする\n",
        "        attn.data.masked_fill_(attn_mask, -float('inf'))\n",
        "        \n",
        "        attn = self.softmax(attn)\n",
        "        attn = self.dropout(attn)\n",
        "        output = torch.bmm(attn, v)\n",
        "\n",
        "        return output, attn"
      ],
      "metadata": {
        "id": "UEap8EJ5RxeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ ScaledDotProductAttention クラス\n",
        "  + temper: 隠れ層次元数の平方根\n",
        "  + torch.bmm: 3次元配列同士の演算の場合，(b,n,m) @ (b,m,p)⇒(b,n,p)  \n",
        "  bのインデックス番号ごとに(m,m)と(m,p)の内積を計算\n",
        "  + torch.tensor.masked_fill_: 単語の入力がない要素は-∞で埋める\n",
        "  + softmaxモジュールを通す\n",
        "  + dropoutモジュールを通す\n",
        "  + Attention配列とvalue配列とのbmm演算を出力とする\n",
        "  + 出力配列とAttention配列を返す"
      ],
      "metadata": {
        "id": "UPeAWBHZR4gH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
        "        \"\"\"\n",
        "        :param n_head: int, ヘッド数\n",
        "        :param d_model: int, 隠れ層の次元数\n",
        "        :param d_k: int, keyベクトルの次元数\n",
        "        :param d_v: int, valueベクトルの次元数\n",
        "        :param dropout: float, ドロップアウト率\n",
        "        \"\"\"\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        self.n_head = n_head\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "\n",
        "        # 各ヘッドごとに異なる重みで線形変換を行うための重み\n",
        "        # nn.Parameterを使うことで、Moduleのパラメータとして登録できる. TFでは更新が必要な変数はtf.Variableでラップするのでわかりやすい\n",
        "        self.w_qs = nn.Parameter(torch.empty([n_head, d_model, d_k], dtype=torch.float))\n",
        "        self.w_ks = nn.Parameter(torch.empty([n_head, d_model, d_k], dtype=torch.float))\n",
        "        self.w_vs = nn.Parameter(torch.empty([n_head, d_model, d_v], dtype=torch.float))\n",
        "        # nn.init.xavier_normal_で重みの値を初期化\n",
        "        nn.init.xavier_normal_(self.w_qs)\n",
        "        nn.init.xavier_normal_(self.w_ks)\n",
        "        nn.init.xavier_normal_(self.w_vs)\n",
        "\n",
        "        self.attention = ScaledDotProductAttention(d_model)\n",
        "        self.layer_norm = nn.LayerNorm(d_model) # 各層においてバイアスを除く活性化関数への入力を平均０、分散１に正則化\n",
        "        self.proj = nn.Linear(n_head*d_v, d_model)  # 複数ヘッド分のAttentionの結果を元のサイズに写像するための線形層\n",
        "        # nn.init.xavier_normal_で重みの値を初期化\n",
        "        nn.init.xavier_normal_(self.proj.weight)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, q, k, v, attn_mask=None):\n",
        "        \"\"\"\n",
        "        :param q: torch.tensor, queryベクトル, \n",
        "            size=(batch_size, len_q, d_model)\n",
        "        :param k: torch.tensor, key, \n",
        "            size=(batch_size, len_k, d_model)\n",
        "        :param v: torch.tensor, valueベクトル, \n",
        "            size=(batch_size, len_v, d_model)\n",
        "        :param attn_mask: torch.tensor, Attentionに適用するマスク, \n",
        "            size=(batch_size, len_q, len_k)\n",
        "        :return outputs: 出力ベクトル, \n",
        "            size=(batch_size, len_q, d_model)\n",
        "        :return attns: Attention\n",
        "            size=(n_head*batch_size, len_q, len_k)\n",
        "            \n",
        "        \"\"\"\n",
        "        d_k, d_v = self.d_k, self.d_v\n",
        "        n_head = self.n_head\n",
        "\n",
        "        # residual connectionのための入力 出力に入力をそのまま加算する\n",
        "        residual = q\n",
        "\n",
        "        batch_size, len_q, d_model = q.size()\n",
        "        batch_size, len_k, d_model = k.size()\n",
        "        batch_size, len_v, d_model = v.size()\n",
        "\n",
        "        # 複数ヘッド化\n",
        "        # torch.repeat または .repeatで指定したdimに沿って同じテンソルを作成\n",
        "        q_s = q.repeat(n_head, 1, 1) # (n_head*batch_size, len_q, d_model)\n",
        "        k_s = k.repeat(n_head, 1, 1) # (n_head*batch_size, len_k, d_model)\n",
        "        v_s = v.repeat(n_head, 1, 1) # (n_head*batch_size, len_v, d_model)\n",
        "        # ヘッドごとに並列計算させるために、n_headをdim=0に、batch_sizeをdim=1に寄せる\n",
        "        q_s = q_s.view(n_head, -1, d_model) # (n_head, batch_size*len_q, d_model)\n",
        "        k_s = k_s.view(n_head, -1, d_model) # (n_head, batch_size*len_k, d_model)\n",
        "        v_s = v_s.view(n_head, -1, d_model) # (n_head, batch_size*len_v, d_model)\n",
        "\n",
        "        # 各ヘッドで線形変換を並列計算(p16左側`Linear`)\n",
        "        q_s = torch.bmm(q_s, self.w_qs)  # (n_head, batch_size*len_q, d_k)\n",
        "        k_s = torch.bmm(k_s, self.w_ks)  # (n_head, batch_size*len_k, d_k)\n",
        "        v_s = torch.bmm(v_s, self.w_vs)  # (n_head, batch_size*len_v, d_v)\n",
        "        # Attentionは各バッチ各ヘッドごとに計算させるためにbatch_sizeをdim=0に寄せる\n",
        "        q_s = q_s.view(-1, len_q, d_k)   # (n_head*batch_size, len_q, d_k)\n",
        "        k_s = k_s.view(-1, len_k, d_k)   # (n_head*batch_size, len_k, d_k)\n",
        "        v_s = v_s.view(-1, len_v, d_v)   # (n_head*batch_size, len_v, d_v)\n",
        "\n",
        "        # Attentionを計算(p16.左側`Scaled Dot-Product Attention * h`)\n",
        "        outputs, attns = self.attention(q_s, k_s, v_s, attn_mask=attn_mask.repeat(n_head, 1, 1))\n",
        "\n",
        "        # 各ヘッドの結果を連結(p16左側`Concat`)\n",
        "        # torch.splitでbatch_sizeごとのn_head個のテンソルに分割\n",
        "        outputs = torch.split(outputs, batch_size, dim=0)  # (batch_size, len_q, d_model) * n_head\n",
        "        # dim=-1で連結\n",
        "        outputs = torch.cat(outputs, dim=-1)  # (batch_size, len_q, d_model*n_head)\n",
        "\n",
        "        # residual connectionのために元の大きさに写像(p16左側`Linear`)\n",
        "        outputs = self.proj(outputs)  # (batch_size, len_q, d_model)\n",
        "        outputs = self.dropout(outputs)\n",
        "        outputs = self.layer_norm(outputs + residual)\n",
        "\n",
        "        return outputs, attns"
      ],
      "metadata": {
        "id": "3ByluYEyH3NW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+  MultiHeadAttentionクラス\n",
        "  + Add&Normモジュールの機能も含む\n",
        "  + attn_mask引数の有無で処理が分かれる(Masked or Non Masked)\n",
        "  + ScaledDotProductAttentionクラスのインスタンスを生成し，\n",
        "  + 入力するquery，key, valueを複製\n",
        "  + 複製したquery, key, valueをまとめてインスタンスに渡す\n",
        "  + 出力配列をバッチサイズで分割\n",
        "  + 出力配列を結合し直す\n",
        "  + 出力配列を入力配列のサイズに戻す\n",
        "  + 入力配列と出力配列の和に対し正規化処理を実施\n",
        "  + 出力配列とAttention配列を返す"
      ],
      "metadata": {
        "id": "yrK3BXajH9F-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    :param d_hid: int, 隠れ層1層目の次元数\n",
        "    :param d_inner_hid: int, 隠れ層2層目の次元数\n",
        "    :param dropout: float, ドロップアウト率\n",
        "    \"\"\"\n",
        "    def __init__(self, d_hid, d_inner_hid, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        # window size 1のconv層を定義することでPosition wiseな全結合層を実現する.\n",
        "        self.w_1 = nn.Conv1d(d_hid, d_inner_hid, 1)\n",
        "        self.w_2 = nn.Conv1d(d_inner_hid, d_hid, 1)\n",
        "        self.layer_norm = nn.LayerNorm(d_hid)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: torch.tensor,\n",
        "            size=(batch_size, max_length, d_hid)\n",
        "        :return: torch.tensor,\n",
        "            size=(batch_size, max_length, d_hid) \n",
        "        \"\"\"\n",
        "        residual = x\n",
        "        output = self.relu(self.w_1(x.transpose(1, 2)))\n",
        "        output = self.w_2(output).transpose(2, 1)\n",
        "        output = self.dropout(output)\n",
        "        return self.layer_norm(output + residual)"
      ],
      "metadata": {
        "id": "doSEANFMOnva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+  PositionwiseFeedForwardクラス\n",
        "  + Add&Normモジュールの機能も含む\n",
        "  + 入力配列を全結合層に通し，活性化関数(ReLU)を通す\n",
        "  + 出力配列を全結合層に通す\n",
        "  + 入力配列と出力配列の和を正規化処理したものを返す"
      ],
      "metadata": {
        "id": "GrziBXigOq1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"Encoderのブロックのクラス\"\"\"\n",
        "    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=0.1):\n",
        "        \"\"\"\n",
        "        :param d_model: int, 隠れ層の次元数\n",
        "        :param d_inner_hid: int, Position Wise Feed Forward Networkの隠れ層2層目の次元数\n",
        "        :param n_head: int,　ヘッド数\n",
        "        :param d_k: int, keyベクトルの次元数\n",
        "        :param d_v: int, valueベクトルの次元数\n",
        "        :param dropout: float, ドロップアウト率\n",
        "        \"\"\"\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        # Encoder内のSelf-Attention\n",
        "        self.slf_attn = MultiHeadAttention(\n",
        "            n_head, d_model, d_k, d_v, dropout=dropout)\n",
        "        # Postionwise FFN\n",
        "        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)\n",
        "\n",
        "    def forward(self, enc_input, slf_attn_mask=None):\n",
        "        \"\"\"\n",
        "        :param enc_input: tensor, Encoderの入力, \n",
        "            size=(batch_size, max_length, d_model)\n",
        "        :param slf_attn_mask: tensor, Self Attentionの行列にかけるマスク, \n",
        "            size=(batch_size, len_q, len_k)\n",
        "        :return enc_output: tensor, Encoderの出力, \n",
        "            size=(batch_size, max_length, d_model)\n",
        "        :return enc_slf_attn: tensor, EncoderのSelf Attentionの行列, \n",
        "            size=(n_head*batch_size, len_q, len_k)\n",
        "        \"\"\"\n",
        "        # Self-Attentionのquery, key, valueにはすべてEncoderの入力（enc_input）が入る\n",
        "        enc_output, enc_slf_attn = self.slf_attn(\n",
        "            enc_input, enc_input, enc_input, attn_mask=slf_attn_mask)\n",
        "        enc_output = self.pos_ffn(enc_output)\n",
        "        return enc_output, enc_slf_attn"
      ],
      "metadata": {
        "id": "9_16fl-xonHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"EncoderLayerブロックからなるEncoderのクラス\"\"\"\n",
        "    def __init__(\n",
        "            self, n_src_vocab, max_length, n_layers=6, n_head=8, d_k=64, d_v=64,\n",
        "            d_word_vec=512, d_model=512, d_inner_hid=1024, dropout=0.1):\n",
        "        \"\"\"\n",
        "        :param n_src_vocab: int, 入力言語の語彙数\n",
        "        :param max_length: int, 最大系列長\n",
        "        :param n_layers: int, レイヤー数\n",
        "        :param n_head: int,　ヘッド数\n",
        "        :param d_k: int, keyベクトルの次元数\n",
        "        :param d_v: int, valueベクトルの次元数\n",
        "        :param d_word_vec: int, 単語の埋め込みの次元数\n",
        "        :param d_model: int, 隠れ層の次元数\n",
        "        :param d_inner_hid: int, Position Wise Feed Forward Networkの隠れ層2層目の次元数\n",
        "        :param dropout: float, ドロップアウト率        \n",
        "        \"\"\"\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        n_position = max_length + 1\n",
        "        self.max_length = max_length\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Positional Encodingを用いたEmbedding\n",
        "        self.position_enc = nn.Embedding(n_position, d_word_vec, padding_idx=PAD)\n",
        "        self.position_enc.weight.data = position_encoding_init(n_position, d_word_vec)\n",
        "\n",
        "        # 一般的なEmbedding\n",
        "        self.src_word_emb = nn.Embedding(n_src_vocab, d_word_vec, padding_idx=PAD)\n",
        "\n",
        "        # EncoderLayerをn_layers個積み重ねる\n",
        "        self.layer_stack = nn.ModuleList([\n",
        "            EncoderLayer(d_model, d_inner_hid, n_head, d_k, d_v, dropout=dropout)\n",
        "            for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, src_seq, src_pos):\n",
        "        \"\"\"\n",
        "        :param src_seq: tensor, 入力系列, \n",
        "            size=(batch_size, max_length)\n",
        "        :param src_pos: tensor, 入力系列の各単語の位置情報,\n",
        "            size=(batch_size, max_length)\n",
        "        :return enc_output: tensor, Encoderの最終出力, \n",
        "            size=(batch_size, max_length, d_model)\n",
        "        :return enc_slf_attns: list, EncoderのSelf Attentionの行列のリスト\n",
        "        \"\"\"\n",
        "        # 一般的な単語のEmbeddingを行う\n",
        "        enc_input = self.src_word_emb(src_seq)\n",
        "        # Positional EncodingのEmbeddingを加算する\n",
        "        enc_input += self.position_enc(src_pos)\n",
        "\n",
        "        enc_slf_attns = []\n",
        "        enc_output = enc_input\n",
        "        # key(=enc_input)のPADに対応する部分のみ1のマスクを作成\n",
        "        enc_slf_attn_mask = get_attn_padding_mask(src_seq, src_seq)\n",
        "\n",
        "        # n_layers個のEncoderLayerに入力を通す\n",
        "        for enc_layer in self.layer_stack:\n",
        "            enc_output, enc_slf_attn = enc_layer(\n",
        "                enc_output, slf_attn_mask=enc_slf_attn_mask)\n",
        "            enc_slf_attns += [enc_slf_attn]\n",
        "\n",
        "        return enc_output, enc_slf_attns"
      ],
      "metadata": {
        "id": "04SydsyKpbXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ EncoderLayerクラス\n",
        "  + MultiHeadAttentionとPositionwiseFeedForwardを結合したクラス\n",
        "  + Self-Attentionで処理\n",
        "+ Encoderクラス\n",
        "  + エンコーダ部分をまとめたクラス\n",
        "  + 入力系列に対しEmbedding\n",
        "  + Positional Encodingを加算\n",
        "  + PAD部分にマスク処理\n",
        "  + EncoderLayerの処理を複数回処理"
      ],
      "metadata": {
        "id": "VD1jRUHGoUbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"Decoderのブロックのクラス\"\"\"\n",
        "    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=0.1):\n",
        "        \"\"\"\n",
        "        :param d_model: int, 隠れ層の次元数\n",
        "        :param d_inner_hid: int, Position Wise Feed Forward Networkの隠れ層2層目の次元数\n",
        "        :param n_head: int,　ヘッド数\n",
        "        :param d_k: int, keyベクトルの次元数\n",
        "        :param d_v: int, valueベクトルの次元数\n",
        "        :param dropout: float, ドロップアウト率\n",
        "        \"\"\"\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        # Decoder内のSelf-Attention\n",
        "        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
        "        # Encoder-Decoder間のSource-Target Attention\n",
        "        self.enc_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
        "        # Positionwise FFN\n",
        "        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)\n",
        "\n",
        "    def forward(self, dec_input, enc_output, slf_attn_mask=None, dec_enc_attn_mask=None):\n",
        "        \"\"\"\n",
        "        :param dec_input: tensor, Decoderの入力, \n",
        "            size=(batch_size, max_length, d_model)\n",
        "        :param enc_output: tensor, Encoderの出力, \n",
        "            size=(batch_size, max_length, d_model)\n",
        "        :param slf_attn_mask: tensor, Self Attentionの行列にかけるマスク, \n",
        "            size=(batch_size, len_q, len_k)\n",
        "        :param dec_enc_attn_mask: tensor, Soutce-Target Attentionの行列にかけるマスク, \n",
        "            size=(batch_size, len_q, len_k)\n",
        "        :return dec_output: tensor, Decoderの出力, \n",
        "            size=(batch_size, max_length, d_model)\n",
        "        :return dec_slf_attn: tensor, DecoderのSelf Attentionの行列, \n",
        "            size=(n_head*batch_size, len_q, len_k)\n",
        "        :return dec_enc_attn: tensor, DecoderのSoutce-Target Attentionの行列, \n",
        "            size=(n_head*batch_size, len_q, len_k)\n",
        "        \"\"\"\n",
        "        # Self-Attentionのquery, key, valueにはすべてDecoderの入力（dec_input）が入る\n",
        "        dec_output, dec_slf_attn = self.slf_attn(\n",
        "            dec_input, dec_input, dec_input, attn_mask=slf_attn_mask)\n",
        "        # Source-Target-AttentionのqueryにはDecoderの出力(dec_output), key, valueにはEncoderの出力（enc_output）が入る\n",
        "        dec_output, dec_enc_attn = self.enc_attn(\n",
        "            dec_output, enc_output, enc_output, attn_mask=dec_enc_attn_mask)\n",
        "        dec_output = self.pos_ffn(dec_output)\n",
        "\n",
        "        return dec_output, dec_slf_attn, dec_enc_attn"
      ],
      "metadata": {
        "id": "0V7Wr6IhrvhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ DecoderLayerクラス\n",
        "  + 2種類のMultiHeadAttentionとPositionwiseFeedForwardを結合したクラス\n",
        "    + エンコーダからの出力はSource-Target Attention\n",
        "    + デコーダへの入力はSelf-Attention\n",
        "    + 出力系列，Self-Attention, Source-Target Attentionそれぞれに対応するデータを出力"
      ],
      "metadata": {
        "id": "kuKMREPZtBc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"DecoderLayerブロックからなるDecoderのクラス\"\"\"\n",
        "    def __init__(\n",
        "            self, n_tgt_vocab, max_length, n_layers=6, n_head=8, d_k=64, d_v=64,\n",
        "            d_word_vec=512, d_model=512, d_inner_hid=1024, dropout=0.1):\n",
        "        \"\"\"\n",
        "        :param n_tgt_vocab: int, 出力言語の語彙数\n",
        "        :param max_length: int, 最大系列長\n",
        "        :param n_layers: int, レイヤー数\n",
        "        :param n_head: int,　ヘッド数\n",
        "        :param d_k: int, keyベクトルの次元数\n",
        "        :param d_v: int, valueベクトルの次元数\n",
        "        :param d_word_vec: int, 単語の埋め込みの次元数\n",
        "        :param d_model: int, 隠れ層の次元数\n",
        "        :param d_inner_hid: int, Position Wise Feed Forward Networkの隠れ層2層目の次元数\n",
        "        :param dropout: float, ドロップアウト率        \n",
        "        \"\"\"\n",
        "        super(Decoder, self).__init__()\n",
        "        n_position = max_length + 1\n",
        "        self.max_length = max_length\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Positional Encodingを用いたEmbedding\n",
        "        self.position_enc = nn.Embedding(\n",
        "            n_position, d_word_vec, padding_idx=PAD)\n",
        "        self.position_enc.weight.data = position_encoding_init(n_position, d_word_vec)\n",
        "\n",
        "        # 一般的なEmbedding\n",
        "        self.tgt_word_emb = nn.Embedding(\n",
        "            n_tgt_vocab, d_word_vec, padding_idx=PAD)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # DecoderLayerをn_layers個積み重ねる\n",
        "        self.layer_stack = nn.ModuleList([\n",
        "            DecoderLayer(d_model, d_inner_hid, n_head, d_k, d_v, dropout=dropout)\n",
        "            for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, tgt_seq, tgt_pos, src_seq, enc_output):\n",
        "        \"\"\"\n",
        "        :param tgt_seq: tensor, 出力系列, \n",
        "            size=(batch_size, max_length)\n",
        "        :param tgt_pos: tensor, 出力系列の各単語の位置情報,\n",
        "            size=(batch_size, max_length)\n",
        "        :param src_seq: tensor, 入力系列, \n",
        "            size=(batch_size, n_src_vocab)\n",
        "        :param enc_output: tensor, Encoderの出力, \n",
        "            size=(batch_size, max_length, d_model)\n",
        "        :return dec_output: tensor, Decoderの最終出力, \n",
        "            size=(batch_size, max_length, d_model)\n",
        "        :return dec_slf_attns: list, DecoderのSelf Attentionの行列のリスト \n",
        "        :return dec_slf_attns: list, DecoderのSelf Attentionの行列のリスト\n",
        "        \"\"\"\n",
        "        # 一般的な単語のEmbeddingを行う\n",
        "        dec_input = self.tgt_word_emb(tgt_seq)\n",
        "        # Positional EncodingのEmbeddingを加算する\n",
        "        dec_input += self.position_enc(tgt_pos)\n",
        "\n",
        "        # Self-Attention用のマスクを作成\n",
        "        # key(=dec_input)のPADに対応する部分が1のマスクと、queryから見たkeyの未来の情報に対応する部分が1のマスクのORをとる\n",
        "        dec_slf_attn_pad_mask = get_attn_padding_mask(tgt_seq, tgt_seq)  # (N, max_length, max_length)\n",
        "        dec_slf_attn_sub_mask = get_attn_subsequent_mask(tgt_seq)  # (N, max_length, max_length)\n",
        "        dec_slf_attn_mask = torch.gt(dec_slf_attn_pad_mask + dec_slf_attn_sub_mask, 0)  # ORをとる\n",
        "\n",
        "        # key(=dec_input)のPADに対応する部分のみ1のマスクを作成\n",
        "        dec_enc_attn_pad_mask = get_attn_padding_mask(tgt_seq, src_seq)  # (N, max_length, max_length)\n",
        "\n",
        "        dec_slf_attns, dec_enc_attns = [], []\n",
        "\n",
        "        dec_output = dec_input\n",
        "        # n_layers個のDecoderLayerに入力を通す\n",
        "        for dec_layer in self.layer_stack:\n",
        "            dec_output, dec_slf_attn, dec_enc_attn = dec_layer(\n",
        "                dec_output, enc_output,\n",
        "                slf_attn_mask=dec_slf_attn_mask,\n",
        "                dec_enc_attn_mask=dec_enc_attn_pad_mask)\n",
        "\n",
        "            dec_slf_attns += [dec_slf_attn]\n",
        "            dec_enc_attns += [dec_enc_attn]\n",
        "\n",
        "        return dec_output, dec_slf_attns, dec_enc_attns"
      ],
      "metadata": {
        "id": "vffbqXXjsN5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Decoderクラス\n",
        "  + デコーダ部分をまとめたクラス\n",
        "  + 入力系列に対しEmbedding\n",
        "  + Positional Encodingを加算\n",
        "  + PAD部分にマスク処理\n",
        "  + 未来の単語をマスク処理\n",
        "  + マスク処理した2つのデータをOR演算\n",
        "  + エンコーダからの入力系列にマスク処理\n",
        "  + DecoderLayerの処理を複数回処理"
      ],
      "metadata": {
        "id": "4XjFYAfSBb7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    \"\"\"Transformerのモデル全体のクラス\"\"\"\n",
        "    def __init__(\n",
        "            self, n_src_vocab, n_tgt_vocab, max_length, n_layers=6, n_head=8,\n",
        "            d_word_vec=512, d_model=512, d_inner_hid=1024, d_k=64, d_v=64,\n",
        "            dropout=0.1, proj_share_weight=True):\n",
        "        \"\"\"\n",
        "        :param n_src_vocab: int, 入力言語の語彙数\n",
        "        :param n_tgt_vocab: int, 出力言語の語彙数\n",
        "        :param max_length: int, 最大系列長\n",
        "        :param n_layers: int, レイヤー数\n",
        "        :param n_head: int,　ヘッド数\n",
        "        :param d_k: int, keyベクトルの次元数\n",
        "        :param d_v: int, valueベクトルの次元数\n",
        "        :param d_word_vec: int, 単語の埋め込みの次元数\n",
        "        :param d_model: int, 隠れ層の次元数\n",
        "        :param d_inner_hid: int, Position Wise Feed Forward Networkの隠れ層2層目の次元数\n",
        "        :param dropout: float, ドロップアウト率        \n",
        "        :param proj_share_weight: bool, 出力言語の単語のEmbeddingと出力の写像で重みを共有する        \n",
        "        \"\"\"\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = Encoder(\n",
        "            n_src_vocab, max_length, n_layers=n_layers, n_head=n_head,\n",
        "            d_word_vec=d_word_vec, d_model=d_model,\n",
        "            d_inner_hid=d_inner_hid, dropout=dropout)\n",
        "        self.decoder = Decoder(\n",
        "            n_tgt_vocab, max_length, n_layers=n_layers, n_head=n_head,\n",
        "            d_word_vec=d_word_vec, d_model=d_model,\n",
        "            d_inner_hid=d_inner_hid, dropout=dropout)\n",
        "        self.tgt_word_proj = nn.Linear(d_model, n_tgt_vocab, bias=False)\n",
        "        nn.init.xavier_normal_(self.tgt_word_proj.weight)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        assert d_model == d_word_vec  # 各モジュールの出力のサイズは揃える\n",
        "\n",
        "        if proj_share_weight:\n",
        "            # 出力言語の単語のEmbeddingと出力の写像で重みを共有する\n",
        "            assert d_model == d_word_vec\n",
        "            self.tgt_word_proj.weight = self.decoder.tgt_word_emb.weight\n",
        "\n",
        "    def get_trainable_parameters(self):\n",
        "        # Positional Encoding以外のパラメータを更新する\n",
        "        enc_freezed_param_ids = set(map(id, self.encoder.position_enc.parameters()))\n",
        "        dec_freezed_param_ids = set(map(id, self.decoder.position_enc.parameters()))\n",
        "        freezed_param_ids = enc_freezed_param_ids | dec_freezed_param_ids\n",
        "        return (p for p in self.parameters() if id(p) not in freezed_param_ids)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_seq, src_pos = src\n",
        "        tgt_seq, tgt_pos = tgt\n",
        "\n",
        "        src_seq = src_seq[:, 1:]\n",
        "        src_pos = src_pos[:, 1:]\n",
        "        tgt_seq = tgt_seq[:, :-1]\n",
        "        tgt_pos = tgt_pos[:, :-1]\n",
        "\n",
        "        enc_output, *_ = self.encoder(src_seq, src_pos)\n",
        "        dec_output, *_ = self.decoder(tgt_seq, tgt_pos, src_seq, enc_output)\n",
        "        seq_logit = self.tgt_word_proj(dec_output)\n",
        "\n",
        "        return seq_logit"
      ],
      "metadata": {
        "id": "bHltbxs2D2um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Transformerクラス\n",
        "  + EncoderとDecoderを順次処理\n",
        "  + 出力に対し線形処理を実行"
      ],
      "metadata": {
        "id": "86xv48dPD6a8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "【参考文献】\n",
        "1. 斎藤康毅著『セロから作るDeepLearning2 自然言語処理編』オライリー・ジャパン 2018.07\n",
        "2. 巣籠悠輔著『詳解ディープラーニング 第2版』マイナビ出版 2019.11"
      ],
      "metadata": {
        "id": "fWQCCbHPE8W1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsj8kM-X8tFy"
      },
      "source": [
        "## Section6: 物体検知・セグメンテーション\n",
        "+ 物体認識  \n",
        "  入力は画像（カラー・白黒は問わず）\n",
        "  + 分類：画像に対してクラスラベルが出力(1or多)⇒物体の有無\n",
        "  + 物体検知：バウンディングボックスが出力⇒物体の位置\n",
        "  + 意味領域分割：各ピクセルにカテゴリラベルの出力\n",
        "  + 個体領域分割：各ピクセルに個体別ラベルの出力⇒個々の物体の位置\n",
        "+ Object Detection\n",
        "  + 入力：画像\n",
        "  + 出力：バウンディングボックス，ラベル，確率(信頼度)\n",
        "+ 代表的なデータセット\n",
        "\n",
        "||ラベル|クラス|Train+Val|Box数|備考|\n",
        "|---|:---:|---:|---:|---:|---|\n",
        "|VOC12|○|20|11,540|2.4|現在は終了|\n",
        "|ILSVRC17||200|476,668|1.1|ImageNetのサブセット．コンペは終了|\n",
        "|MS COCO18|○|80|123,287|7.3|マイクロソフト|\n",
        "|OICOD18|○|500|1,743,042|7.0|OpenImagesV4のサブセット|\n",
        "\n",
        "  + 目的に応じてデータセットを選択\n",
        "    + Box数\n",
        "    + クラス数が多いのが正義？\n",
        "\n",
        "+ 評価指標\n",
        "  + Confution Matrix\n",
        "    + precision: 正と予測したものがどれだけ正しいか\n",
        "    + recall: 実際に正であるものをどれだけ予測できたか\n",
        "  + Precision-Recall Curve\n",
        "    + confidenceの閾値を変化させてグラフ化\n",
        "  + 閾値変化による物体検知\n",
        "    + 閾値が変化してもクラス分類ではサンプル数は変わらない\n",
        "    + 物体検知では検知される物体の数が変わる\n",
        "  + 物体位置の予測\n",
        "    + IoU: Intersection over Union(Jaccard係数)\n",
        "$$\n",
        "IoU = \\frac{TP}{TP+FP+FN}=\\frac{\\text{Area of Overlap}}{\\text{Area of Union}}\n",
        "$$\n",
        "    + 同じ物体を複数回検出した場合はconfとIoUが閾値を超えた中で最も数値の高いものをTPとなり，それ以外はFPとなる\n",
        "    + 検出できない物体はFNとなる\n",
        "    + AP: Avrage Precision\n",
        "      + IoUの値を固定したときのPRcueveをp(r)とすると\n",
        "      $$\n",
        "      AP = \\int_0^1 p(r) dr\n",
        "      $$\n",
        "    + mAP: mean Average Precision  \n",
        "    全てのクラスに対するAP平均\n",
        "    $$\n",
        "    mAP = \\frac{1}{C} \\sum_{i=1}^{C} AP_i\n",
        "    $$\n",
        "    + IoUも0.5から0.05刻みで計算(MS COCO)\n",
        "    $$\n",
        "    mAP_{coco} = \\frac{1}{10} \\sum_{j=0.5}^{0.95}\\frac{1}{C} \\sum_{i=1}^{C} AP_{j,i}\n",
        "    $$\n",
        "    + 検出速度\n",
        "      + FPS: Frames per Second：1秒当たりの処理枚数\n",
        "    + 速度と精度の関係性も検討\n",
        "+ 深層学習以降の物体検知\n",
        "  + 2段階検出器：候補領域とクラス推定を別々に行う  \n",
        "  候補領域の特定⇒画像切り出し⇒クラス推定  \n",
        "  RCNN, SPPNet, Fast RCNN, Faster RCNN, RFCN, FPN, Mask RCNN\n",
        "    + 精度が高い\n",
        "    + 計算量が大きく推論も遅い\n",
        "\n",
        "  + 1段階検出器：候補領域とクラス推定を同時に行う  \n",
        "  DetectorNet, SSD, YOLO, YOLO9000, RetinaNet, CornerNet\n",
        "    + 精度が低い\n",
        "    + 計算量が小さく推論も速い"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ SSD: Single Shot Detector\n",
        "  + デフォルトボックスを設定\n",
        "    + 1つの特徴量に複数のデフォルトボックスを用意\n",
        "    + k個のデフォルトボックスを用意するとk×(クラス数＋オフセット4)×マップサイズm×n\n",
        "  + 処理を繰り返し検出したい物体に当てはまるボックスを見つける\n",
        "+ SSD300\n",
        "  + input:(3,300,300)\n",
        "  + conv4_3:(512,38,38) デフォルトボックス: 4×(21＋4)×38×38 = 5776×(21+4)\n",
        "  + conv7:(1024,19,19) デフォルトボックス: 6×(21＋4)×19×19 = 2166×(21+4)\n",
        "  + conv8_2:(512,10,10) デフォルトボックス: 6×(21＋4)×10×10 = 600×(21+4)\n",
        "  + conv9_2:(256,5,5) デフォルトボックス: 6×(21＋4)×5×5 = 150×(21+4)\n",
        "  + conv10_2:(256,3,3) デフォルトボックス: 4×(21＋4)×3×3 = 36×(21+4)\n",
        "  + conv11_2:(256,1,1) デフォルトボックス: 4×(21＋4)×1×1 = 4×(21+4)\n",
        "+ 特徴マップの解像度が高いと小さいものを検知しやすい\n",
        "+ 複数のデフォルトボックスへの対応\n",
        "  + Non-Maximum Suppression: 閾値以上で最大の信頼度のものに絞る\n",
        "  + Hard Negative Mining: 背景部分の学習に制限をつける(物体の3倍まで）\n",
        "+ 損失関数\n",
        "  + 信頼度と位置の2項目に関する損失を計算する"
      ],
      "metadata": {
        "id": "0ekp664uNxRn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Semantic Segmentation(DeconvNet・SegNet)\n",
        "  + 畳み込みとプーリングで解像度が落ちた状態からどのようにUP-Samplingするか  \n",
        "  ⇒Deconvolution(転置畳み込み)\n",
        "\n",
        "    + 特徴マップの間に隙間を空ける\n",
        "    + 特徴マップの周りに余白を入れる\n",
        "    + 畳み込み演算を行う\n",
        "    + 輪郭情報の補完  \n",
        "  解像度が落ちた状態から元の解像度に戻すときにローカル(輪郭)情報が欠落している\n",
        "    + 途中段階の出力を組み合わせて情報を補完\n",
        "    + FCN(Fully Convolutional Network): Up-samplingと低レイヤの要素ごとに加算\n",
        "    + U-Net: Up-samplingと低レイヤチャネル方向で結合  \n",
        "\n",
        "    ⇒Unpooling\n",
        "\n",
        "    + プーリング時前の位置を保持\n",
        "  + Dilated Convolution(拡張畳み込み)\n",
        "    + 畳み込み演算の段階で受容野を広げられる"
      ],
      "metadata": {
        "id": "JYgYmHYHSfva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "【参考文献】\n",
        "1. チーム・カルポ著『物体検出とＧＡＮ、オートエンコーダー、画像処理入門 』秀和システム 2021.08"
      ],
      "metadata": {
        "id": "oOCV50yqSEgi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DCGAN\n",
        "+ GAN(Generative Adversarial Nets)\n",
        "  + Generator: 乱数からデータを生成\n",
        "  + Discriminator: 入力データが学習データか生成データかを識別\n",
        "  + 2プレイヤーミニマックスゲーム\n",
        "    + 生成器は識別器に学習データである判断させようとトレーニング\n",
        "    + 識別器は正しく識別できるようにトレーニング\n",
        "$$\n",
        "    \\underset{G}{min}~ \\underset{D}{max} V(D, G)\\\\\n",
        "    V(D, G) = \\mathbb{E}_{x\\sim p_{data(x)}}[\\log D(x)]+\\mathbb{E}_{z\\sim p_{z(z)}}[1 - \\log D(G(z))]\n",
        "$$\n",
        "  + 価値観数の最適化\n",
        "    + 価値観数はバイナリクロスエントロピーと同等\n",
        "    + 識別器は生成器のパラメータを固定して勾配上昇法を使う(max)\n",
        "      + 学習には学習データと生成データを利用して複数回更新\n",
        "      + 識別器はノイズと学習データから価値観数を演算\n",
        "    + 生成器は識別器のパラメータを固定して勾配降下法を使う(min)\n",
        "      + 学習には生成データを利用して1回更新\n",
        "      + 生成器にはノイズから生成データを作る\n",
        "    + 最適化されると生成の確率分布と学習データの確率分布が等しくなるはず  \n",
        "\n",
        "      ここでx=G(z)とすると，価値関数を最大化するD(x)は\n",
        "$$\n",
        "D(x) = \\frac{p_{data(x)}}{p_{data(x)} + p_{g(x)}}\n",
        "$$\n",
        "これを価値関数の式に代入すると\n",
        "$$\n",
        "V = \\mathbb{E}_{x\\sim p_{data(x)}} \\left [ \\log\\frac{p_{data(x)}}{p_{data(x)} + p_{g(x)}} \\right ] +\\mathbb{E}_{z\\sim p_{z(z)}}\\left [1 - \\log \\frac{p_{g(x)}}{p_{data(x)} + p_{g(x)}} \\right ]\n",
        "= 2JS(p_{data} || ~p_{g}) -2 \\log2\n",
        "$$\n",
        "JSダイバージェンスは非負で2つの確率分布が等しいときに0となり，このときが最小\n",
        "\n",
        "+ DCGAN(Deep Convolutional GAN)\n",
        "  + 畳み込み深層学習を使った画像生成モデル\n",
        "  + 学習の安定化策\n",
        "    + プーリング層は使わず，識別器にはストライド2の畳み込み層，生成器にはストライド2の転置畳み込み層を使用\n",
        "    + 生成器の出力層と識別器の入力層以外でバッチ正規化を使用\n",
        "    + 中間層は畳み込み層のみでのネットワーク構成\n",
        "    + 生成器の活性化関数は出力層でTanh，それ以外はReLU\n",
        "    + 識別器の活性化関数は全てLeakyReLU"
      ],
      "metadata": {
        "id": "pORmvxF5aIv5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "【参考文献】\n",
        "1. チーム・カルポ著『物体検出とＧＡＮ、オートエンコーダー、画像処理入門 』秀和システム 2021.08"
      ],
      "metadata": {
        "id": "Lzzu-h0zvGZe"
      }
    }
  ]
}